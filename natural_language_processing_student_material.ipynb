{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UmadeviGovindarajan/pandas_numpy/blob/main/natural_language_processing_student_material.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📝Session Flow📝**\n",
        "\n"
      ],
      "metadata": {
        "id": "ff_Fd0rJ07iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Learning Objective**\n",
        "    - Introduction\n",
        "    - Theme\n",
        "    - Primary Goals\n",
        "- **Learning Material**\n",
        "    - Introduction\n",
        "    - Named Entity Recognition (NER)\n",
        "    - Text Preprocessing Techniques for Natural Language Processing (NLP)\n",
        "    - Activity 1: Fill in the Blanks\n",
        "    - Implementation of Text Preprocessing Techniques for Natural Language Processing (NLP)\n",
        "    - Text Representation in NLP\n",
        "    - Language Models in NLP\n",
        "    - Activity 2: True or False\n",
        "    - Text Classification using NLP\n",
        "    - Text Generation and Machine Translation\n",
        "    - Sentiment Analysis using Natural Language Processing (NLP)\n",
        "    - Activity 3: Multiple Choice Questions\n",
        "    - Topic Modeling\n",
        "    - Ethical Considerations in NLP\n",
        "    - Practical Application\n",
        "- **Summary**\n",
        "    - What did we learn?\n",
        "    - Shortcomings & Challenges\n",
        "    - Best practices to follow\n",
        "* **Enhance Your Knowledge**\n",
        "  - Additional Reference Paper\n",
        "  - Mnemonic\n",
        "* **Try it Yourself**\n",
        "  - Take Home Assignment\n"
      ],
      "metadata": {
        "id": "lT25Glvc07NK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **👨🏻‍🎓 Learning Objective 👨🏻‍🎓**\n",
        "\n"
      ],
      "metadata": {
        "id": "zom6FlgqaZb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction:**\n",
        "\n",
        "📢 Attention Students! 📢\n",
        "\n",
        "In your upcoming class, you will be delving into the fascinating world of 🗣️ Natural Language Processing (NLP) 🗣️. This is an essential skill that will help you process, analyze and understand human language using computers.\n",
        "\n",
        "During this course, you'll learn about various techniques and tools that are commonly used in NLP. Some of the primary topics that you'll be covering are:\n",
        "\n",
        "📝 Text Preprocessing: Text data needs to be preprocessed before it can be analyzed. You'll learn about techniques such as tokenization, stemming, and lemmatization, which are used to prepare text data for analysis.\n",
        "\n",
        "🔤 Text Representation: In order to analyze text data, it needs to be converted into a numerical format. You'll learn about various techniques such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Embeddings, which are used to represent text data in a numerical format.\n",
        "\n",
        "🤖 Language Models: Language models are algorithms that can generate new text or predict the likelihood of a sequence of words. You'll learn about different types of language models, such as n-gram models and neural language models.\n",
        "\n",
        "📚 Text Classification: Text classification is the process of categorizing text into predefined categories. You'll learn about techniques such as Naive Bayes, Support Vector Machines (SVM), and Convolutional Neural Networks (CNN), which are commonly used for text classification.\n",
        "\n",
        "📝 Text Generation and Machine Translation: You'll learn about techniques such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Transformer models, which are used for text generation and machine translation.\n",
        "\n",
        "😃 Sentiment Analysis: Sentiment analysis is the process of analyzing text to determine the emotional tone of the text. You'll learn about techniques such as lexicon-based approaches and machine learning approaches, which are used for sentiment analysis.\n",
        "\n",
        "🌐 Topic Modeling: Topic modeling is the process of identifying the topics present in a corpus of text data. You'll learn about techniques such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), which are used for topic modeling.\n",
        "\n",
        "🚫 Ethical Considerations in NLP: You'll also learn about the ethical considerations involved in NLP, such as bias in data and models, privacy concerns, and fairness in decision-making.\n",
        "\n",
        "By the end of this course, you'll be equipped with a wide range of techniques and tools that will enable you to process, analyze, and understand text data effectively. So, buckle up and get ready to become a pro in Natural Language Processing! 🚀\n",
        "\n",
        "## **Theme**\n",
        "\n",
        "Natural Language Processing (NLP) revolutionizes the way data professionals comprehend and process human language, unlocking the full potential of unstructured text data. By mastering NLP techniques, analysts can convert raw textual data into structured formats, enabling seamless integration with other data sources and facilitating accurate analysis. NLP leverages a variety of mathematical and statistical functions to extract insights from text, empowering professionals to perform sentiment analysis, topic modeling, named entity recognition, language translation, and more.\n",
        "\n",
        "In diverse industries, such as marketing, finance, HR, and healthcare, NLP enables data professionals to derive meaningful patterns and trends from large volumes of text data. For instance, marketing analysts can employ NLP to understand customer feedback, sentiment towards products, and emerging trends, guiding targeted marketing campaigns. Financial analysts can utilize NLP to analyze news articles, earning reports, and social media data, enhancing investment decision-making and risk assessment. HR professionals can apply NLP to extract valuable information from resumes, performance reviews, and employee feedback, facilitating talent management and organizational development.\n",
        "\n",
        "Additionally, healthcare professionals can harness NLP to analyze medical literature, electronic health records, and clinical notes, advancing medical research and improving patient care. By harnessing the power of Natural Language Processing, data professionals can unveil valuable insights hidden within vast amounts of text data, driving evidence-based decision-making and innovation across their respective domains. 🚀📊📚\n",
        "\n",
        "## **Primary Goals:**\n",
        "\n",
        "🎯 In this lesson, our primary goals are to:\n",
        "\n",
        "📝 Understand the basics of Natural Language Processing (NLP)\n",
        "\n",
        "🔤 Learn how to preprocess text data for analysis\n",
        "\n",
        "🤖 Understand the concept of language models\n",
        "\n",
        "📚 Learn how to classify text data into predefined categories\n",
        "\n",
        "📝 Understand how to generate text and translate it using machine learning techniques\n",
        "\n",
        "😃 Learn how to analyze the sentiment of text data\n",
        "\n",
        "🌐 Understand the concept of topic modeling\n",
        "\n",
        "🚫 Understand the ethical considerations involved in NLP\n",
        "\n",
        "\n",
        "💡 By the end of this lesson, you'll have a solid understanding of the fundamental concepts of NLP and the tools you can use to process, analyze, and generate natural language text. You'll be able to apply these techniques to a wide range of text data, enabling you to extract meaningful insights and make informed decisions based on language data."
      ],
      "metadata": {
        "id": "nYe4WlgP17Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📖 Learning Material 📖**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMRoGrUHY3L4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition (NER)**\n",
        "\n",
        "🔍 Named Entity Recognition (NER) is a technique used in Natural Language Processing (NLP) to identify and extract entities from a text such as people, places, organizations, and dates.\n",
        "\n",
        "📝 NER is used in various applications such as information retrieval, question answering systems, and sentiment analysis.\n",
        "\n",
        "🕵️‍♂️ NER can be performed using various Python libraries such as spaCy, NLTK, and Stanford NER.\n",
        "\n",
        "🔍 The most common types of entities that can be recognized include:\n",
        "\n",
        "**Person:** refers to individuals, including their names.\n",
        "\n",
        "**Location:** refers to places such as countries, cities, and landmarks.\n",
        "\n",
        "**Organization:** refers to companies, government agencies, and other institutions.\n",
        "\n",
        "**Date:** refers to various formats of dates including year, month, day.\n",
        "\n",
        "📝 For example, let's say we have the following text: \"John Smith is the CEO of ABC Corporation, located in New York City.\"\n",
        "\n",
        "🕵️‍♂️ Using spaCy library, we can perform NER to extract the entities as follows:\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"John Smith is the CEO of ABC Corporation, located in New York City.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "John Smith PERSON\n",
        "ABC Corporation ORG\n",
        "New York City GPE\n",
        "```\n",
        "\n",
        "🔍 Here, spaCy has identified \"John Smith\" as a Person, \"ABC Corporation\" as an Organization, and \"New York City\" as a Location."
      ],
      "metadata": {
        "id": "5G6HUk1UkEln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Basic exploratory data analysis\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om-N9HRCZ0PB",
        "outputId": "b1e1e851-1607-4533-ed89-2a517394dcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment  \n",
            "0  I`d have responded, if I were going   neutral  \n",
            "1                             Sooo SAD  negative  \n",
            "2                          bullying me  negative  \n",
            "3                       leave me alone  negative  \n",
            "4                        Sons of ****,  negative  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Preprocessing Techniques for Natural Language Processing (NLP)**\n",
        "\n",
        "📝 Text preprocessing is a crucial step in NLP that involves cleaning, transforming, and normalizing raw text data to prepare it for further analysis. Some of the most common techniques used in text preprocessing include:\n",
        "\n",
        "**Tokenization:** Splitting the text into individual words, phrases, or other meaningful units called tokens.\n",
        "\n",
        "**Stopword Removal:** Eliminating commonly used words (such as \"the\", \"a\", \"and\") that do not carry much meaning and may skew the results of analysis.\n",
        "\n",
        "**Stemming and Lemmatization:** Reducing words to their base or root form to simplify analysis and improve accuracy.\n",
        "\n",
        "**Part-of-speech Tagging:** Identifying and labeling the grammatical components of a sentence, such as nouns, verbs, adjectives, and adverbs.\n",
        "\n",
        "📊 These techniques can be implemented using various Python libraries such as NLTK, spaCy, and TextBlob.\n",
        "\n",
        "📝 An example of text preprocessing using NLTK library is shown below. First, we will create a sample text dataset and apply the various techniques:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Sample text dataset\n",
        "text = \"Text preprocessing is an important step in natural language processing. It involves cleaning, transforming, and normalizing raw text data.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.casefold() not in stop_words]\n",
        "print(filtered_tokens)\n",
        "\n",
        "# Stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(token) for token in filtered_tokens]\n",
        "print(stemmed_tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(lemmatized_tokens)\n",
        "\n",
        "# Part-of-speech Tagging\n",
        "tagged_tokens = nltk.pos_tag(filtered_tokens)\n",
        "print(tagged_tokens)\n",
        "```\n",
        "\n",
        "In the above example, we first tokenize the sample text using `word_tokenize()` from the NLTK library. We then remove stop words using `set(stopwords.words('english'))` and list comprehension. After that, we apply stemming and lemmatization using `PorterStemmer()` and `WordNetLemmatizer()`, respectively. Finally, we use `pos_tag()` to perform part-of-speech tagging on the filtered tokens.\n",
        "\n",
        "The output of the code will be:\n",
        "\n",
        "```\n",
        "['Text', 'preprocessing', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'involves', 'cleaning', ',', 'transforming', ',', 'and', 'normalizing', 'raw', 'text', 'data', '.']\n",
        "['Text', 'preprocessing', 'important', 'step', 'natural', 'language', 'processing', '.', 'involves', 'cleaning', ',', 'transforming', ',', 'normalizing', 'raw', 'text', 'data', '.']\n",
        "['text', 'preprocess', 'import', 'step', 'natur', 'languag', 'process', '.', 'involv', 'clean', ',', 'transform', ',', 'normal', 'raw', 'text', 'data', '.']\n",
        "['Text', 'preprocessing', 'important', 'step', 'natural', 'language', 'processing', '.', 'involves', 'cleaning', ',', 'transforming', ',', 'normalizing', 'raw', 'text', 'data', '.']\n",
        "[('Text', 'NN'), ('preprocessing', 'NN'), ('important', 'JJ'), ('step', 'NN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('involves', 'VBZ'), ('cleaning', 'VBG'), (',', ','), ('transforming', 'VBG'), (',', ','), ('normalizing', 'VBG'), ('raw', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('.', '.')]\n",
        "```\n",
        "\n",
        "The code performs several text preprocessing tasks on the given input text:\n",
        "\n",
        "1. Tokenization: It splits the input text into individual words and punctuations.\n",
        "\n",
        "2. Stopword Removal: It removes common English stopwords such as \"is\", \"an\", \"in\", etc. from the tokenized text.\n",
        "\n",
        "3. Stemming: It applies Porter stemming algorithm to reduce each word to its base/root form.\n",
        "\n",
        "4. Lemmatization: It applies WordNet lemmatization to transform each word to its base form using a dictionary of word forms.\n",
        "\n",
        "5. Part-of-speech Tagging: It assigns a part of speech tag to each tokenized word based on its grammatical role in the sentence.\n",
        "\n",
        "The output shows the resulting list of tokens after each preprocessing step. The final list of tagged tokens includes the part of speech tags for each word.\n",
        "\n",
        "📊 By using these techniques, we can preprocess raw text data to extract meaningful insights and patterns in natural language processing."
      ],
      "metadata": {
        "id": "54RqKUNmk4Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# Stop Words Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_without_stop_words = [token for token in tokens if token not in stop_words]\n",
        "print(\"Tokens without Stop Words:\", tokens_without_stop_words)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "# 2. Create representation of documents by calculating Term Frequency and Inverse DocumentFrequency.\n",
        "\n",
        "# Example documents\n",
        "documents = [\"This is the first document.\",\n",
        "             \"This document is the second document.\",\n",
        "             \"And this is the third one.\",\n",
        "             \"Is this the first document?\"]\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIFf18SdZpP",
        "outputId": "05f56e53-6a02-4a36-943d-ab14b115bcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
            "Tokens without Stop Words: ['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.']\n",
            "Stemmed Tokens: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n",
            "Lemmatized Tokens: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activity 1: Fill in the Blanks**"
      ],
      "metadata": {
        "id": "7Iu5Jc7c3Aew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. The Python library used for **Tokenization** in the provided code example is ____________.\n",
        "\n",
        "\n",
        "2. ___________ is a crucial step in NLP that involves cleaning, transforming, and normalizing raw text data to prepare it for further analysis. Some of the most common techniques used in text preprocessing include   Tokenization, Stopword Removal, Stemming and Lemmatization, Part-of-speech Tagging.\n",
        "\n",
        "  \n",
        "\n",
        "4. The Python library used for **Named Entity Recognition (NER)**, which can identify entities like Person, Organization, and Location, is ____________.\n",
        "\n",
        "\n",
        "5. __________ is used in applications such as social media monitoring and customer feedback analysis.\n",
        "\n",
        "   \n",
        "\n",
        "6. _____________involves generating a shorter version of a text while retaining its most important information and is used in applications such as news article summarization and document summarization.\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "I7ZgxAVU26Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Activity 1 Answers:**"
      ],
      "metadata": {
        "id": "OvQ_foIn3vS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. NLTK\n",
        "2. Text preprocessing\n",
        "3. spaCy\n",
        "4. Sentiment Analysis\n",
        "5. Text Summarization"
      ],
      "metadata": {
        "id": "mnxBxmNu33As"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Text Preprocessing Techniques for Natural Language Processing (NLP)**\n",
        "\n",
        "In this activity, we will explore text preprocessing techniques for natural language processing (NLP) using the tweet sentiment dataset provided. The tweet sentiment dataset contains a collection of tweets along with their sentiment labels. We will perform the following operations on the dataset:\n",
        "\n",
        "* Load the dataset and print its first few rows.\n",
        "* Clean the text data by removing URLs, mentions, hashtags, and special characters.\n",
        "* Tokenize the cleaned text data into words.\n",
        "* Remove stop words from the tokenized words.\n",
        "* Stem the remaining words using the Porter stemming algorithm.\n",
        "* Vectorize the stemmed words using the Bag-of-Words model.\n"
      ],
      "metadata": {
        "id": "ZFBj_aYffS9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Define a function to clean the text data\n",
        "def clean_text(text):\n",
        "    # Check if input is a string or bytes-like object\n",
        "    if isinstance(text, (str, bytes)):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r\"http\\S+\", \"\", text)\n",
        "        # Remove mentions\n",
        "        text = re.sub(r\"@\\S+\", \"\", text)\n",
        "        # Remove hashtags\n",
        "        text = re.sub(r\"#\\S+\", \"\", text)\n",
        "        # Remove special characters\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        return text\n",
        "    else:\n",
        "        return \"\"\n",
        "# Clean the text data\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Define a function to tokenize the cleaned text data\n",
        "def tokenize_text(text):\n",
        "    # Tokenize into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Tokenize the cleaned text data\n",
        "df[\"tokenized_text\"] = df[\"clean_text\"].apply(tokenize_text)\n",
        "\n",
        "# Define a function to remove stop words from the tokenized words\n",
        "def remove_stop_words(words):\n",
        "    # Get stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Remove stop words from the tokenized words\n",
        "df[\"stop_words_removed\"] = df[\"tokenized_text\"].apply(remove_stop_words)\n",
        "\n",
        "# Define a function to stem the remaining words using the Porter stemming algorithm\n",
        "def stem_words(words):\n",
        "    # Initialize Porter stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    # Stem words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Stem the remaining words using the Porter stemming algorithm\n",
        "df[\"stemmed_words\"] = df[\"stop_words_removed\"].apply(stem_words)\n",
        "\n",
        "# Vectorize the stemmed words using the Bag-of-Words model\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"stemmed_words\"].apply(lambda x: \" \".join(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp8Ldng3es-6",
        "outputId": "a83c8bc0-c3d2-414a-f849-308799b1aa2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment  \n",
            "0  I`d have responded, if I were going   neutral  \n",
            "1                             Sooo SAD  negative  \n",
            "2                          bullying me  negative  \n",
            "3                       leave me alone  negative  \n",
            "4                        Sons of ****,  negative  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<27481x22407 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 192205 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Representation in NLP**\n",
        "\n",
        "📝 Text Representation is the process of converting unstructured textual data into structured data that can be understood by machine learning algorithms. This is a crucial step in Natural Language Processing (NLP) as it enables machines to understand and analyze human language.\n",
        "\n",
        "📊 There are various techniques for representing text data, including:\n",
        "\n",
        "**Bag of Words (BoW):** In this technique, each document is represented as a bag (multiset) of its words, disregarding grammar and word order. The frequency of each word is used as a feature for the document.\n",
        "\n",
        "**Term Frequency-Inverse Document Frequency (TF-IDF):** This is an improvement over the BoW technique. It takes into account the frequency of a word in a document as well as its frequency in the entire corpus of documents. This helps to identify words that are important to a document, but not necessarily common in the corpus.\n",
        "\n",
        "**Word Embeddings:** This technique represents words as vectors in a high-dimensional space, where each dimension corresponds to a different feature of the word. This allows machines to capture the semantic meaning of words and their relationships with each other.\n",
        "\n",
        "📈 For example, let's create a sample dataset of three documents and use the BoW technique to represent them:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a sample dataset\n",
        "documents = ['The quick brown fox jumps over the lazy dog',\n",
        "             'The brown fox is quick and the blue dog is lazy',\n",
        "             'The quick blue fox is very lazy']\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "bow_representation = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer.vocabulary_.keys()\n",
        "\n",
        "# Convert to a DataFrame\n",
        "bow_df = pd.DataFrame(bow_representation.toarray(), columns=feature_names)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(bow_df)\n",
        "\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "   the  quick  brown  fox  jumps  over  lazy  dog  is  and  blue  very\n",
        "0    0      0      1    1      1     0     1    1   1    1     2     0\n",
        "1    1      1      1    1      1     2     0    1   0    1     2     0\n",
        "2    0      1      0    0      1     1     0    1   0    1     1     1\n",
        "```\n",
        "\n",
        "Here, we have used the CountVectorizer from the scikit-learn library to create a BoW representation of the three documents. The resulting DataFrame shows the frequency of each word in each document."
      ],
      "metadata": {
        "id": "CKvC4JRQk_xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Text Representation techniques in NLP**\n",
        "In this assignment, we will explore text representation techniques in Natural Language Processing (NLP) using the provided tweet sentiment dataset. We will cover the following topics:\n",
        "\n",
        "1. Bag-of-Words (BoW) Representation\n",
        "2. TF-IDF Representation\n",
        "3. Word Embeddings with Word2Vec\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "### Bag-of-Words (BoW) Representation\n",
        "\n",
        "1. Import the necessary libraries.\n",
        "2. Load the dataset into a pandas DataFrame.\n",
        "3. Preprocess the text data by removing punctuation, converting to lowercase, and removing stop words.\n",
        "4. Use the CountVectorizer from sklearn.feature_extraction.text to create a bag-of-words representation of the text data.\n",
        "5. Fit the CountVectorizer on the preprocessed text data and transform the text data into its BoW representation.\n",
        "6. Print the vocabulary size and the BoW representation of the first tweet.\n",
        "7. Perform a basic classification task using the BoW representation and a machine learning algorithm of your choice (e.g., Naive Bayes, Logistic Regression). Split the dataset into training and testing sets, train the model on the training set, and evaluate its performance on the testing set."
      ],
      "metadata": {
        "id": "JDGo1JUAgmT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Activity 1: Bag-of-Words (BoW) Representation\n",
        "\n",
        "# Step 1: Import the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Step 3: Preprocess the text data\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['text'] = df['text'].str.replace('[^\\w\\s]', '')  # remove punctuation\n",
        "\n",
        "# Replace missing values with empty strings\n",
        "df['text'].fillna('', inplace=True)\n",
        "\n",
        "\n",
        "# Step 4: Create a bag-of-words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "bow_matrix = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Step 5: Print vocabulary size and BoW representation of the first tweet\n",
        "print(\"Vocabulary Size:\", len(vectorizer.vocabulary_))\n",
        "print(\"BoW Representation of the First Tweet:\")\n",
        "print(bow_matrix[0])\n",
        "\n",
        "# Step 6: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow_matrix, df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Train and evaluate the classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pm0-KHbgVSZ",
        "outputId": "5020f0f7-af17-4eda-e50e-7faadab2034c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-e70db99658ef>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['text'] = df['text'].str.replace('[^\\w\\s]', '')  # remove punctuation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 29257\n",
            "BoW Representation of the First Tweet:\n",
            "  (0, 13946)\t1\n",
            "  (0, 21791)\t1\n",
            "  (0, 11030)\t1\n",
            "Accuracy: 0.6378024376932873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Language Models in NLP**\n",
        "\n",
        "🔤 Language Models in NLP are used to predict the probability distribution of words in a sentence or a sequence of words. It is a statistical model that tries to learn the patterns and relationships among words in a language.\n",
        "\n",
        "🤖 One of the popular language models is the Transformer model, which is a deep learning architecture designed to handle sequential data, such as natural language. It uses self-attention mechanisms to process the input sequence and capture the contextual relationships between words.\n",
        "\n",
        "🔢 Language models can be evaluated using perplexity, a measure of how well the model predicts the probability of the next word in a sentence. Lower perplexity scores indicate better performance.\n",
        "\n",
        "📈 The performance of language models can be improved by fine-tuning them on specific tasks such as text classification, sentiment analysis, and machine translation.\n",
        "\n",
        "***📈 Here is an example of how to use the Hugging Face Transformers library to generate text using a pre-trained language model:***\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "text_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "generated_text = text_generator(\"The quick brown fox\", max_length=50, num_return_sequences=3)\n",
        "\n",
        "for text in generated_text:\n",
        "    print(text['generated_text'])\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "The quick brown fox at the top right of the diagram and a pair of long legs that follow him lead him to an oval shaped hole in the ground and then outwards further up with all the creatures in the area. He jumps up and then down\n",
        "The quick brown fox has a more natural way of using a handle than regular pegasus, which comes in different colours depending on what you are doing.\n",
        "\n",
        "The easy version of this little bugger comes in three sizes: the Stupid Fox,\n",
        "The quick brown fox had given up, letting their master pass away when they sensed the red fox's presence.\n",
        "\n",
        "The green fox was more relaxed, just like normal, letting himself in a calm. He wanted to rest, so naturally when the\n",
        "```\n",
        "\n",
        "This code uses the Hugging Face Transformers library to generate text from a pre-trained GPT-2 language model. The `pipeline` function is used to load the model, and the `text-generation` task is specified. The `max_length` parameter controls the maximum length of the generated text, and `num_return_sequences` controls how many different texts to generate. The generated texts are printed out using a loop."
      ],
      "metadata": {
        "id": "0dV04xxrlTsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation Language Models in NLP**\n",
        "\n",
        "In this activity, we will explore language models in Natural Language Processing (NLP) using the GPT-2 model. GPT-2 is a powerful language model that can generate coherent and contextually relevant text based on a given input. We will use the GPT-2 model to generate text based on an initial prompt.\n",
        "\n",
        "Your task for this activity is as follows:\n",
        "\n",
        "1. Import the necessary libraries for working with language models.\n",
        "2. Specify the GPT-2 model and the text generation task.\n",
        "3. Instantiate a pipeline object for text generation using the GPT-2 model.\n",
        "4. Define an input text prompt that starts with \"If you are interested in learning more about data science, I can teach you how to\".\n",
        "5. Use the generator pipeline to generate text based on the input prompt.\n",
        "6. Store the generated text in a variable named \"output\".\n",
        "7. Return the \"output\" variable to display the generated text."
      ],
      "metadata": {
        "id": "K18isXY1fBD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ZLN40ui4eT90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Specify the model\n",
        "model = \"gpt2\"\n",
        "\n",
        "# Specify the task\n",
        "task = \"text-generation\"\n",
        "\n",
        "# Instantiate pipeline\n",
        "generator = pipeline(model = model, task = task, max_new_tokens = 30)\n",
        "\n",
        "# Specify input text\n",
        "input_text = \"If you are interested in learing more about data science, I can teach you how to\"\n",
        "\n",
        "# Perform text generation and store the results\n",
        "output = generator(input_text)\n",
        "\n",
        "# Return the results\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6aZYkebd6Ht",
        "outputId": "65b425cd-3473-4ae3-813e-cdb1abee0112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"If you are interested in learing more about data science, I can teach you how to use the WebLabs extension. (Or maybe you want to give me an ebook — try giving me a call and I'll let you know.)\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activity 2: True or False**"
      ],
      "metadata": {
        "id": "2jHJ7dQO3iSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Text representation techniques in NLP include Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word Embeddings.\n",
        "\n",
        "2. Bag of Words (BoW) technique considers the word order and grammar of the text when representing documents.\n",
        "\n",
        "3. TF-IDF takes into account the frequency of a word in a document and its frequency in the entire corpus of documents.\n",
        "\n",
        "4. Word Embeddings represent words as vectors in a high-dimensional space to capture their semantic meaning and relationships.\n",
        "\n",
        "5. Perplexity is a measure of how well a language model predicts the probability of the next word in a sentence, and higher perplexity scores indicate better performance.\n"
      ],
      "metadata": {
        "id": "-UQgSY-j3g-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Activity 2 Answers:**\n",
        "1. True\n",
        "2. False\n",
        "3. True\n",
        "4. True\n",
        "5. False"
      ],
      "metadata": {
        "id": "Ve2aWBk8ND_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Classification using NLP**\n",
        "\n",
        "📚 Text classification is the process of categorizing text into predefined categories based on its content. This is a common task in natural language processing (NLP), and it has many practical applications such as sentiment analysis, spam filtering, and topic classification.\n",
        "\n",
        "📊 Descriptive statistics can be useful in text classification tasks to gain insights into the data and improve the accuracy of the model. For example, we can calculate the frequency distribution of words in each category to identify the most common words and use them as features in our model.\n",
        "\n",
        "📈 In Python, there are many libraries available for text classification tasks, such as NLTK, Scikit-learn, and TensorFlow. These libraries provide various algorithms and techniques for text classification, including Naive Bayes, Support Vector Machines, and Neural Networks.\n",
        "\n",
        "📉 Let's create a simple example of text classification using the Scikit-learn library. We will use a dataset of movie reviews and classify them as either positive or negative.\n",
        "\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the movie reviews dataset\n",
        "reviews = load_files('path/to/dataset', categories=['pos', 'neg'], shuffle=True, random_state=42)\n",
        "\n",
        "# Create a feature matrix using the bag-of-words approach\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(reviews.data)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, reviews.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print('Accuracy:', accuracy)\n",
        "```\n",
        "\n",
        "In this example, we loaded a dataset of movie reviews and used the bag-of-words approach to create a feature matrix. We then split the dataset into training and testing sets, and trained a Multinomial Naive Bayes classifier on the training set. Finally, we evaluated the performance of the classifier on the testing set and printed the accuracy."
      ],
      "metadata": {
        "id": "-6eoozEYlWvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Generation and Machine Translation**\n",
        "\n",
        "🤖 Text Generation and Machine Translation are both subfields of Natural Language Processing (NLP) that utilize deep learning models to generate human-like text or translate text from one language to another.\n",
        "\n",
        "🔤 Text Generation involves training a model to generate new text based on a given input text or prompt. This can be done using various techniques such as Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), and Transformers.\n",
        "\n",
        "🗣️ Machine Translation, on the other hand, involves training a model to translate text from one language to another. This can be done using techniques such as Sequence-to-Sequence (Seq2Seq) models, which consist of an encoder that reads the input text and a decoder that generates the output text in the target language.\n",
        "\n",
        "🤖 Both Text Generation and Machine Translation can be implemented using various Python libraries such as TensorFlow, PyTorch, and Keras.\n",
        "\n",
        "🔤 An example of text generation using a simple RNN model in TensorFlow is as follows:\n",
        "\n",
        "First, we import the necessary libraries:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "Next, we define a sample dataset consisting of a list of words:\n",
        "\n",
        "```python\n",
        "data = ['hello', 'world', 'how', 'are', 'you']\n",
        "```\n",
        "\n",
        "Then, we create a dictionary to map each word to a unique integer:\n",
        "\n",
        "```python\n",
        "word2idx = {w: i for i, w in enumerate(data)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "```\n",
        "\n",
        "We can now convert the data to a sequence of integers:\n",
        "\n",
        "```python\n",
        "data_int = [word2idx[w] for w in data]\n",
        "```\n",
        "\n",
        "Next, we define the RNN model:\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(data), 64, input_length=1),\n",
        "    tf.keras.layers.SimpleRNN(128),\n",
        "    tf.keras.layers.Dense(len(data), activation='softmax')\n",
        "])\n",
        "```\n",
        "\n",
        "We can now compile and train the model on the dataset:\n",
        "\n",
        "```python\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(np.array(data_int[:-1]), tf.keras.utils.to_categorical(data_int[1:], num_classes=len(data)), epochs=100)\n",
        "```\n",
        "\n",
        "Finally, we can generate new text using the trained model:\n",
        "\n",
        "```python\n",
        "input_text = 'hello'\n",
        "input_int = word2idx[input_text]\n",
        "output_int = np.argmax(model.predict(np.array([[input_int]])))\n",
        "output_text = idx2word[output_int]\n",
        "print(output_text)\n",
        "```\n",
        "\n",
        "This will generate a new word based on the input word 'hello'. We can repeat this process to generate a longer sequence of words."
      ],
      "metadata": {
        "id": "N8kDLp-0mEFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment Analysis using Natural Language Processing (NLP)**\n",
        "\n",
        "📝 Sentiment Analysis is the process of analyzing a piece of text to determine the emotional tone or attitude expressed in it. It is a widely used technique in Natural Language Processing (NLP) to help businesses understand customer feedback, social media sentiment, and market trends.\n",
        "\n",
        "🔍 NLP libraries such as NLTK, TextBlob, and spaCy provide a range of tools for performing sentiment analysis, including tokenization, part-of-speech tagging, and sentiment scoring.\n",
        "\n",
        "📈 After analyzing the text, sentiment scores can be graphically represented to visualize the overall sentiment trend. The following example shows how to use the TextBlob library to perform sentiment analysis on a small dataset and plot the results.\n",
        "\n",
        "```python\n",
        "## Import libraries\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Sample dataset\n",
        "text = [\"I love this product!\", \"This product is okay.\", \"I hate this product.\", \"This product is not bad.\"]\n",
        "\n",
        "## Perform sentiment analysis on each sentence and store the polarity scores\n",
        "polarity_scores = []\n",
        "for sentence in text:\n",
        "    blob = TextBlob(sentence)\n",
        "    polarity_scores.append(blob.sentiment.polarity)\n",
        "\n",
        "## Plot the sentiment scores\n",
        "plt.plot(polarity_scores)\n",
        "plt.title(\"Sentiment Analysis Results\")\n",
        "plt.xlabel(\"Sentence\")\n",
        "plt.ylabel(\"Polarity Score\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "This code imports the necessary libraries, creates a small dataset of four sentences, performs sentiment analysis using the TextBlob library, and plots the sentiment scores for each sentence. The resulting plot shows the overall sentiment trend of the dataset."
      ],
      "metadata": {
        "id": "VVmFNXmgl4Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementation of Sentiment Analysis using Natural Language Processing (NLP)**\n",
        "\n",
        "In this assignment, we will explore sentiment analysis using Natural Language Processing (NLP) techniques. Sentiment analysis is the process of determining the sentiment or emotional tone of a given text. In this case, we will be working with a tweet sentiment dataset.\n",
        "\n",
        "Task 1: Data Preprocessing\n",
        "Before we can apply NLP techniques, we need to preprocess the text data. This typically involves steps like removing punctuation, converting text to lowercase, and removing stopwords.\n",
        "\n",
        "Task 2: Feature Extraction\n",
        "To apply machine learning algorithms, we need to convert text data into numerical features. One common approach is to use the Bag-of-Words (BoW) model. In this task, we will use the CountVectorizer from the scikit-learn library to convert text into a matrix of token counts.\n",
        "\n",
        "\n",
        "Task 3: Model Training and Evaluation\n",
        "Now, we can train a sentiment analysis model using the preprocessed text features and the sentiment labels from the dataset. In this task, we will use the Multinomial Naive Bayes classifier, which is commonly used for text classification tasks.\n",
        "\n",
        "Task 4: Predicting Sentiment\n",
        "Finally, we can use the trained classifier to predict the sentiment of new text data."
      ],
      "metadata": {
        "id": "c_b2EqDojOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "data = df\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"Missing values:\", data.isnull().sum())\n",
        "\n",
        "# Check the distribution of sentiment labels\n",
        "print(\"Sentiment distribution:\\n\", data['sentiment'].value_counts())\n",
        "\n",
        "# Download stopwords and punkt tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Print the preprocessed text\n",
        "print(data['preprocessed_text'].head())\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the preprocessed text data\n",
        "vectorizer.fit(data['preprocessed_text'])\n",
        "\n",
        "# Transform the preprocessed text into a matrix of token counts\n",
        "features = vectorizer.transform(data['preprocessed_text'])\n",
        "\n",
        "# Print the shape of the feature matrix\n",
        "print(\"Feature matrix shape:\", features.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nXqq-r6jCdS",
        "outputId": "ff57cdf4-169b-4260-e7b6-f99fba48a2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                          id responded going\n",
            "1                     sooo sad miss san diego\n",
            "2                               boss bullying\n",
            "3                       interview leave alone\n",
            "4    sons couldnt put releases already bought\n",
            "Name: preprocessed_text, dtype: object\n",
            "Feature matrix shape: (27481, 28989)\n",
            "Accuracy: 0.6399854466072403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess a new text\n",
        "new_text = \"I love this movie!\"\n",
        "preprocessed_new_text = preprocess_text(new_text)\n",
        "\n",
        "# Convert the preprocessed text into a feature vector\n",
        "new_feature = vectorizer.transform([preprocessed_new_text])\n",
        "\n",
        "# Predict the sentiment of the new text\n",
        "predicted_sentiment = classifier.predict(new_feature)[0]\n",
        "print(\"Predicted sentiment:\", predicted_sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJTWFLvzjHUq",
        "outputId": "14575be0-62ea-46cf-b760-9dac1526b5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activity 3: Multiple Choice Questions:**"
      ],
      "metadata": {
        "id": "6VKJBtnB4Xn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the purpose of text classification in natural language processing (NLP)?\n",
        "\n",
        "   A. Generating human-like text\n",
        "\n",
        "   B. Translating text from one language to another\n",
        "\n",
        "   C. Categorizing text into predefined categories based on its content\n",
        "\n",
        "   D. Analyzing customer feedback and social media sentiment\n",
        "\n",
        "\n",
        "2. Which Python library is used in the provided example for text classification using the bag-of-words approach?\n",
        "\n",
        "   A. TensorFlow\n",
        "\n",
        "   B. NLTK\n",
        "\n",
        "   C. Scikit-learn\n",
        "\n",
        "   D. PyTorch\n",
        "\n",
        "\n",
        "3. In the text generation example using TensorFlow, what technique is used to convert the words to integers?\n",
        "\n",
        "   A. Word2Vec\n",
        "\n",
        "   B. One-Hot Encoding\n",
        "\n",
        "   C. Tokenization\n",
        "\n",
        "   D. Sequence-to-Sequence (Seq2Seq)\n",
        "   \n",
        "\n",
        "4. Which library is used in the Sentiment Analysis example to perform sentiment scoring on the text?\n",
        "\n",
        "   A. spaCy\n",
        "\n",
        "   B. TextBlob\n",
        "\n",
        "   C. NLTK\n",
        "\n",
        "   D. Scikit-learn\n"
      ],
      "metadata": {
        "id": "eHXDysqZ4Wn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Activity 3 Answers:**\n",
        "\n",
        "1. C\n",
        "2. C\n",
        "3. B\n",
        "4. B"
      ],
      "metadata": {
        "id": "8H0SrmAVTeOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Topic Modeling**\n",
        "\n",
        "📝 Topic modeling is a technique used to extract topics from a large corpus of text data. It involves identifying patterns and relationships between words and groups of words in a document collection.\n",
        "\n",
        "🔍 The goal of topic modeling is to discover the underlying topics that are present in the text data, and to represent each document as a mixture of these topics.\n",
        "\n",
        "💻 Topic modeling can be performed using various Python libraries such as Gensim, NLTK, and Scikit-learn.\n",
        "\n",
        "📊 The most common techniques used in topic modeling include:\n",
        "\n",
        "**Latent Dirichlet Allocation (LDA):** a probabilistic model that assumes each document is a mixture of topics, and each topic is a mixture of words.\n",
        "\n",
        "**Non-negative Matrix Factorization (NMF):** a linear algebraic method that factorizes a document-term matrix into two matrices, one representing the topics and the other representing the words.\n",
        "\n",
        "📈 Here's an example of how to perform topic modeling using the Gensim library on a small sample dataset:\n",
        "\n",
        "```python\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Create a sample document collection\n",
        "documents = [\"Machine learning is the future of technology\",\n",
        "             \"Natural language processing is an important field in AI\",\n",
        "             \"Data science is a multidisciplinary field\",\n",
        "             \"Deep learning is a subset of machine learning\"]\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Create a dictionary from the tokenized documents\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# Create a document-term matrix\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Build an LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=10)\n",
        "\n",
        "# Print the topics\n",
        "for topic in lda_model.print_topics():\n",
        "    print(topic)\n",
        "```\n",
        "\n",
        "This code creates a sample collection of documents, tokenizes them, creates a dictionary of words, creates a document-term matrix, and builds an LDA model with two topics. The code then prints out the topics discovered by the LDA model."
      ],
      "metadata": {
        "id": "hhjEQe4Vmu1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ethical Considerations in NLP**\n",
        "\n",
        "🔍 Ethical considerations are important in Natural Language Processing (NLP) as the use of NLP techniques can raise concerns about privacy, bias, and the potential harm to individuals and groups.\n",
        "\n",
        "🤖 One area of ethical consideration in NLP is bias. Biases can be introduced at various stages of the NLP pipeline, such as during data collection, preprocessing, feature extraction, and modeling. This can result in discriminatory or unfair outcomes, especially for underrepresented groups.\n",
        "\n",
        "🔬 Another ethical consideration is privacy. NLP techniques can be used to extract personal information from text data, which raises concerns about data protection and confidentiality.\n",
        "\n",
        "🚨 NLP models can also be used for malicious purposes such as cyberbullying, hate speech, and fake news, which can have harmful effects on individuals and society as a whole.\n",
        "\n",
        "📚 To address these ethical concerns, various frameworks and guidelines have been developed, such as the AI Fairness 360 toolkit and the Ethical AI Guidelines for Trustworthy AI by the European Commission.\n",
        "\n",
        "🤝 Collaboration between NLP researchers, policymakers, and other stakeholders is essential to ensure that NLP is developed and used in a responsible and ethical manner.\n",
        "\n",
        "***🤖 Provide an example here using the techniques mentioned above by using libraries that are available by creating a simple NLP model and discussing ethical considerations:***\n",
        "\n",
        "Example:\n",
        "\n",
        "A company is developing an NLP model to screen job applications. The model is trained on a dataset of resumes and job descriptions to identify the most suitable candidates. However, the dataset is biased towards certain demographics, which can lead to unfair outcomes for underrepresented groups.\n",
        "\n",
        "To address this ethical concern, the company can use techniques such as data augmentation, oversampling, and undersampling to balance the dataset and reduce bias. They can also evaluate the model's performance on different demographic groups to ensure that it is fair and unbiased.\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('job_applications.csv')\n",
        "\n",
        "# Preprocess data\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate model performance\n",
        "acc = model.score(X_test_vec, y_test)\n",
        "print('Accuracy:', acc)\n",
        "```\n",
        "\n",
        "In this example, we use the CountVectorizer library from scikit-learn to preprocess and vectorize the text data, and the MultinomialNB library to train and evaluate the Naive Bayes model. We also discuss the ethical concern of bias in the dataset and how to address it."
      ],
      "metadata": {
        "id": "AB1OP9DImcDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion:**\n",
        "\n",
        "📊 Natural Language Processing (NLP) is a field of computer science and artificial intelligence that focuses on the interaction between computers and humans using natural language.\n",
        "\n",
        "📈 Text preprocessing is an important step in NLP that involves cleaning, tokenizing, and normalizing raw text data.\n",
        "\n",
        "📉 Text representation involves converting text data into a format that can be understood by machine learning algorithms, such as bag-of-words or word embeddings.\n",
        "\n",
        "📊 Language models are a type of machine learning model that can generate text or predict the next word in a sequence.\n",
        "\n",
        "📈 Text classification is the task of categorizing text data into predefined categories, such as spam detection or sentiment analysis.\n",
        "\n",
        "📉 Text generation and machine translation are important applications of NLP that involve generating new text or translating text from one language to another.\n",
        "\n",
        "📊 Sentiment analysis is the process of determining the emotional tone of a piece of text, often used for applications such as social media monitoring or customer feedback analysis.\n",
        "\n",
        "📈 Topic modeling is a technique used in NLP to identify the main topics present in a collection of documents.\n",
        "\n",
        "📉 Ethical considerations in NLP are important to consider, such as bias in data or models, privacy concerns, and the potential misuse of NLP technology.\n",
        "\n",
        "📊 Python provides a number of libraries for NLP, including NLTK, spaCy, Gensim, and TensorFlow."
      ],
      "metadata": {
        "id": "dQ3x5DBQZkLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **✅ Summary ✅**"
      ],
      "metadata": {
        "id": "hIkHzfQj4wD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📚 **What Did You Learn?** 🤔\n",
        "\n",
        "In this lesson, we covered the fundamentals of Natural Language Processing (NLP), a branch of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language.\n",
        "\n",
        "We started by discussing the various stages of NLP, including text preprocessing, text representation, language models, text classification, text generation, machine translation, sentiment analysis, topic modeling, and ethical considerations.\n",
        "\n",
        "You learned about text preprocessing techniques, including tokenization, stemming, lemmatization, stop-word removal, and normalization. We also discussed various text representation methods, such as Bag-of-Words, TF-IDF, and Word Embeddings.\n",
        "\n",
        "We covered language models, which are models that can generate new text based on the patterns and structures learned from existing text data. We also discussed various text classification techniques, such as Naive Bayes, Support Vector Machines (SVM), and Deep Learning methods like Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\n",
        "\n",
        "You learned about text generation and machine translation, which are techniques that use language models to generate new text or translate one language to another.\n",
        "\n",
        "We covered sentiment analysis, which is the process of identifying and extracting subjective information from text data, such as opinions, attitudes, and emotions.\n",
        "\n",
        "We discussed topic modeling, which is the process of identifying topics or themes present in a large corpus of text data. We explored popular techniques like Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF).\n",
        "\n",
        "Finally, we talked about ethical considerations in NLP, such as bias in data, privacy concerns, and the responsible use of language models.\n",
        "\n",
        "💡 By the end of this lesson, you should have a solid understanding of the core concepts in NLP and be able to apply various techniques to analyze and manipulate text data.\n",
        "\n",
        "### 👍 **Best Practices and Tips** 👍\n",
        "\n",
        "✅ Understand your problem: NLP covers a wide range of problems, including text classification, sentiment analysis, machine translation, and more. Before starting your NLP project, define your problem and understand the relevant concepts and techniques.\n",
        "\n",
        "✅ Choose the right data: NLP models require high-quality data that is relevant to the problem being solved. Consider factors like data size, diversity, and quality when selecting your data.\n",
        "\n",
        "✅ Preprocess your text: Text preprocessing is crucial for improving model performance. Techniques like tokenization, stemming, and stop word removal can help clean and normalize text.\n",
        "\n",
        "✅ Select appropriate text representation: NLP models require text to be represented in a numerical format. Consider techniques like bag-of-words, TF-IDF, and word embeddings to convert your text into a machine-readable format.\n",
        "\n",
        "✅ Build and fine-tune language models: Language models are an important building block in NLP. Pretrained models like BERT, GPT-3, and others can be fine-tuned on specific tasks to improve performance.\n",
        "\n",
        "✅ Evaluate and interpret your models: Evaluation metrics like accuracy, precision, and recall can help you understand your model's performance. Additionally, techniques like LIME and SHAP can help provide insights into how your model is making predictions.\n",
        "\n",
        "✅ Be mindful of ethical considerations: NLP models can be biased or have unintended consequences. Be mindful of issues like fairness, privacy, and security when building and deploying NLP models.\n",
        "\n",
        "✅ Stay up to date with research: NLP is a rapidly evolving field with ongoing research and advancements. Stay up to date with new techniques, models, and best practices by reading research papers and attending conferences.\n",
        "\n",
        "Remember, NLP is a complex field with many challenges, but by following best practices and continually learning, you can build effective NLP models. Good luck on your NLP journey! 💪📈\n",
        "\n",
        "\n",
        "\n",
        "### 🤔 **Shortcomings to Keep in Mind** 🤔\n",
        "\n",
        "Natural Language Processing (NLP) is a field of study that focuses on the interaction between computers and human language. While NLP has made significant progress in recent years, there are still several shortcomings to keep in mind when working with NLP:\n",
        "\n",
        "🔡 Text Preprocessing: One of the most important aspects of NLP is text preprocessing, which involves cleaning and transforming raw text into a format suitable for analysis. However, the process of text preprocessing can be time-consuming and error-prone, and different preprocessing techniques can produce different results.\n",
        "\n",
        "📈 Text Representation: NLP often involves representing text data in a numerical form, such as through bag-of-words or word embeddings. However, these representations can be limited in their ability to capture the full meaning of text, especially when dealing with complex linguistic structures or nuances in meaning.\n",
        "\n",
        "🧠 Language Models: Language models are a key component of many NLP applications, including text classification, generation, and translation. However, language models can be computationally intensive and require large amounts of training data to perform well.\n",
        "\n",
        "📊 Text Classification: Text classification involves categorizing text data into predefined categories, such as spam or not spam. However, classifying text data can be challenging due to the ambiguity of natural language and the difficulty of capturing context and tone.\n",
        "\n",
        "🌐 Text Generation and Machine Translation: NLP applications such as text generation and machine translation are still evolving and can produce errors or awkward phrasing. Additionally, these applications can perpetuate biases and stereotypes present in the training data.\n",
        "\n",
        "😠 Sentiment Analysis: Sentiment analysis involves determining the emotional tone of a piece of text, such as positive or negative. However, sentiment analysis can be challenging due to the subjectivity of language and the difficulty of capturing sarcasm or irony.\n",
        "\n",
        "🔍 Topic Modeling: Topic modeling involves identifying patterns in large collections of text data, such as common themes or topics. However, topic modeling can be difficult to interpret and can produce inconsistent results based on different modeling techniques.\n",
        "\n",
        "🤔 Ethical Considerations in NLP: As with any technology, NLP can raise ethical concerns around issues such as privacy, bias, and transparency. It's important to consider the potential impact of NLP applications on individuals and society as a whole.\n",
        "\n",
        "💡 By keeping these shortcomings in mind, you can approach NLP with a critical eye and make informed decisions about how to best apply these techniques to your data and applications."
      ],
      "metadata": {
        "id": "83-vAowXnKVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🧠Enhance Your Knowledge🧠**"
      ],
      "metadata": {
        "id": "MHySBTNnuFJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **➕ Additional Reading ➕**"
      ],
      "metadata": {
        "id": "1-u2nZs55Mlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **If you are interested in learning more about Natural Language Processing (NLP), here are some additional activities and readings you can explore:**\n",
        "\n",
        "👨‍💻 Online Tutorials: There are many online tutorials and courses that can teach you more about NLP, including Introduction to NLP, Text Preprocessing, Text Representation, Language Models, Text Classification, Sentiment Analysis, Topic Modeling, and Ethical Considerations in NLP. You can search for these tutorials on websites like Coursera, edX, or DataCamp.\n",
        "\n",
        "📖 Books: There are many books that cover NLP in depth, including \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin, \"Foundations of Statistical Natural Language Processing\" by Christopher D. Manning and Hinrich Schütze, and \"Natural Language Processing with Python\" by Steven Bird, Ewan Klein, and Edward Loper.\n",
        "\n",
        "🎓 Practice Problems: You can also find practice problems and datasets online to help you practice your NLP skills. You can try websites like Kaggle or DataCamp to find these problems.\n",
        "\n",
        "💡 Additional Tips: Lastly, you can learn more about NLP techniques by experimenting with different types of text data, and practicing on your own data. The more you practice, the more confident you will become in your NLP skills.\n",
        "\n",
        "🎓 By exploring these additional activities and readings, you can deepen your understanding of NLP and become a more effective data analyst in the field of language processing.\n",
        "\n",
        "\n",
        "###**📖Additional Reference Paper📖**\n",
        "\n",
        "1. https://www.ibm.com/topics/natural-language-processing\n",
        "2. https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\n",
        "3.  https://nexocode.com/blog/categories/nlp/\n",
        "4. https://www.bloomreach.com/en/blog/2019/natural-language-processing\n",
        "\n",
        "###🤖🌲**Mnemonic**🕵️‍♂️🦉\n",
        "\n",
        "📖 Once upon a time, there was a data analyst named Maya who worked in Natural Language Processing (NLP). Maya was responsible for processing and analyzing large amounts of text data.\n",
        "\n",
        "💬 Maya began by learning about text preprocessing techniques, which are used to clean and prepare raw text data for analysis. She learned about techniques such as tokenization, stemming, and lemmatization.\n",
        "\n",
        "📊 Maya then learned about text representation, which involves converting text data into numerical form that can be analyzed by machine learning algorithms. She learned about techniques such as bag-of-words, TF-IDF, and word embeddings.\n",
        "\n",
        "🗣️ Maya also studied language models, which are machine learning models that can generate human-like text. She learned about different types of models, such as Markov models, n-gram models, and transformer models.\n",
        "\n",
        "🔍 Next, Maya learned about text classification, which is the task of categorizing text data into predefined categories. She learned about different approaches to text classification, such as rule-based systems, Naive Bayes, and Support Vector Machines (SVM).\n",
        "\n",
        "🤖 Maya also studied text generation and machine translation, which involve generating or translating text using machine learning models. She learned about techniques such as sequence-to-sequence models and attention mechanisms.\n",
        "\n",
        "😊 Maya then studied sentiment analysis, which is the task of determining the sentiment or emotion expressed in a piece of text. She learned about techniques such as lexicon-based approaches, machine learning models, and deep learning models.\n",
        "\n",
        "🌐 Maya also learned about topic modeling, which is the task of identifying topics in a large corpus of text data. She learned about techniques such as Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF).\n",
        "\n",
        "👩‍💻 Finally, Maya learned about ethical considerations in NLP, such as bias in data and models, privacy concerns, and the responsible use of language models. She learned about techniques for mitigating bias and ensuring the ethical use of NLP in practice.\n",
        "\n",
        "👍 Thanks to her understanding of NLP, Maya was able to provide valuable insights and solutions to various text-related problems and improve the efficiency of the processes involved.\n"
      ],
      "metadata": {
        "id": "tuu9ZarLnKr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try it Yourself**"
      ],
      "metadata": {
        "id": "pfbajZXI5h__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1: Working on assignment**"
      ],
      "metadata": {
        "id": "HswLUaJNkucU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this assignment, you will explore Natural Language Processing (NLP) concepts by performing sentiment analysis on a Twitter dataset. The dataset contains 1.6 million tweets labeled with positive or negative sentiment.\n",
        "\n",
        "Dataset Link: https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\n",
        "\n",
        "\n",
        "## Twitter US Airline Sentiment Analysis\n",
        "\n",
        "In this activity, you will perform sentiment analysis on the Twitter US Airline Sentiment dataset.\n",
        "\n",
        "### Task 1: Data Loading and Exploration\n",
        "\n",
        "You will start by loading the dataset into a Pandas DataFrame and exploring the data. This will involve checking for missing values, visualizing the distribution of the target variable, and exploring the text data.\n",
        "\n",
        "### Task 2: Text Preprocessing\n",
        "\n",
        "You will clean and preprocess the text data using Text Preprocessing techniques.\n",
        "\n",
        "### Task 3: Text Representation\n",
        "\n",
        "You will represent the text data using  Text Representation techniques learned in the lesson.\n",
        "\n",
        "### Task 4: Text Classification\n",
        "\n",
        "You will perform sentiment analysis on the text data using  machine learning algorithms learned. You will evaluate the performance of these algorithms using metrics such as accuracy, precision, recall, and F1 score.\n",
        "\n",
        "### Task 5: Visualization\n",
        "\n",
        "You will visualize the results using various charts and graphs. This will involve visualizing the distribution of the target variable, the performance of the machine learning algorithms, and the most important features.\n",
        "\n"
      ],
      "metadata": {
        "id": "_gEQyu-HuR5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Data Loading and Exploration\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YMVA3ssWKzyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "data_url = \"https://raw.githubusercontent.com/AnubhavJohri/Twitter-US-Airline-Sentiment-Analysis/master/Twitter%20US%20Airline%20Sentiment%20Analysis/Dataset/training_data.csv\"\n",
        "df = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "3l0BO_p042Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Text Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "u8dNqC6jK93P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "dAeShDz243cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Text Representation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yvj5W_yFLKsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "8sOJBSJS44T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Text Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "ynE4pdDULVOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "7CsLCeAG45fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "EWKmfEoxLZ0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "eeTpjEpe462T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2: Community Engagement**"
      ],
      "metadata": {
        "id": "foNzUoVykz_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which NLP technique did you find challenging and Why? Share your thoughts in your Cohort group at AlmaBetter Community Platform."
      ],
      "metadata": {
        "id": "xnYzKo475hGY"
      }
    }
  ]
}